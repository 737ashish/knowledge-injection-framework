{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3c1512-3de5-4da7-a74a-f67eafc7cc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "sys.path.extend([\"../utils\", \"../evaluate\"])\n",
    "from metrics import compute_n_gram_entropy\n",
    "from wikidata import load_wikidata_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7943c23-399b-45bd-b505-94882f44f600",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # disable tokenizer warning\n",
    "\n",
    "# set up logging\n",
    "formatter = logging.Formatter(fmt=\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "handler = logging.StreamHandler()\n",
    "handler.setFormatter(formatter)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "transformers.utils.logging.set_verbosity_error()\n",
    "transformers.utils.logging.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffddfb8a-2ec5-4a86-ad78-262489ad58f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"results/pipeline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadab51d-2a90-40a8-ac63-9b36805a6d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "files = sorted([file for file in os.listdir(output_dir) if \"json\" in file])\n",
    "logger.info(f\"Found {len(files)} files in {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e15852-c745-47c8-a1d4-73c11c1fc68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3709d4a7-2b6d-4941-a4b4-f20f19ce7fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "fname = files[n]\n",
    "fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02604e74-dec9-433a-ae88-2edf0da7b295",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"{n+1}/{len(files)} Evaluating {fname}\")\n",
    "_, dataset, split, method, model_name = fname.split(\".\")[0].split(\"_\")\n",
    "\n",
    "# load data\n",
    "df_gen = pd.read_json(f\"{output_dir}/{fname}\", lines=True)\n",
    "df_gt = pd.read_json(f\"../datasets/{dataset}/{dataset}_{split}.json\", lines=True)\n",
    "df = df_gt.merge(df_gen)\n",
    "wikidata_dict = load_wikidata_json(\"../datasets/wikidata_entity_data.json\")\n",
    "logger.info(f\"Loaded dataframe with {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6012f6-ce31-4cd8-a6c4-f3bf9c69bc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(row):\n",
    "    gen = sent_tokenize(row[\"gen\"])[0][len(row[\"prompt\"]):]\n",
    "    aliases = wikidata_dict[row[f\"{row['cf_entity_type']}_id\"]][\"aliases\"]\n",
    "    if aliases:\n",
    "            aliases = set(sum([x.split(\", \") for x in aliases], []))\n",
    "    if row[f\"{row['cf_entity_type']}_retrieved\"] in gen or (aliases and any([a in gen for a in aliases])):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "df[\"acc\"] = df.apply(lambda row: accuracy(row), axis=1)\n",
    "np.round(df[\"acc\"].mean() * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e61e46e-8440-4ec2-8468-ec3a5573859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_pipeline(row):\n",
    "    gen = sent_tokenize(row[\"gen_pipeline\"])[0][len(row[\"prompt\"]):]\n",
    "    aliases = wikidata_dict[row[f\"{row['cf_entity_type']}_id\"]][\"aliases\"]\n",
    "    if aliases:\n",
    "            aliases = set(sum([x.split(\", \") for x in aliases], []))\n",
    "    if row[f\"{row['cf_entity_type']}_retrieved\"] in gen or (aliases and any([a in gen for a in aliases])):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "df[\"acc_pipeline\"] = df.apply(lambda row: accuracy_pipeline(row), axis=1)\n",
    "np.round(df[\"acc_pipeline\"].mean() * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46d8596-6401-4c35-991d-c324647b72d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_edit\n",
    "np.round((~df[\"acc\"] == df[\"correction\"]).mean() * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c67110-b28e-4b1f-86fc-05306750292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_retrieval(row):\n",
    "    if row[\"triple_retrieved\"]:\n",
    "        entity_id_retrieved = row[\"triple_retrieved\"][\"retrieved\"][0][1]\n",
    "        if entity_id_retrieved == row[f\"{row['cf_entity_type']}_id\"]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "df[\"acc_retrieval\"] = df.apply(lambda row: accuracy_retrieval(row), axis=1)\n",
    "\n",
    "# acc_retrieval_all\n",
    "np.round(df[df[\"correction\"]][\"acc_retrieval\"].mean() * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f13305-2788-4288-a5f8-2bf0b84a238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def injection_accuracy(row):\n",
    "    gen = sent_tokenize(row[\"gen_pipeline\"])[0][len(row[\"prompt\"]):]\n",
    "    if row[\"triple_retrieved\"]:\n",
    "        ent = row[\"triple_retrieved\"][\"retrieved\"][0][0]\n",
    "        id = row[\"triple_retrieved\"][\"retrieved\"][0][1]\n",
    "        aliases = wikidata_dict.get(id)\n",
    "        if aliases:\n",
    "                aliases = aliases[\"aliases\"]\n",
    "                if aliases:\n",
    "                    aliases = set(sum([x.split(\", \") for x in aliases], []))\n",
    "        if ent in gen or (aliases and any([a in gen for a in aliases])):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "df[\"acc_injection_all\"] = df.apply(lambda row: injection_accuracy(row), axis=1)\n",
    "np.round(df[df[\"correction\"]][\"acc_injection_all\"].mean() * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff0cc14-20bd-4472-a009-ff8f8e821955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fluency_ngram_entropy(df):\n",
    "    fluency_pre = df.apply(lambda row: compute_n_gram_entropy(sent_tokenize(row[\"gen\"])[0]), axis=1)\n",
    "    fluency_post = df.apply(lambda row: compute_n_gram_entropy(sent_tokenize(row[\"gen_pipeline\"])[0]), axis=1)\n",
    "    return ((fluency_post - fluency_pre) / fluency_pre).mean()\n",
    "\n",
    "np.round(fluency_ngram_entropy(df) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb50cafc-c94c-4d5b-b352-83c50c447336",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"textattack/roberta-base-CoLA\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cde59e8-f25b-488b-ab6f-cb2e69f2de63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_acceptability(sentences, tokenizer, model):\n",
    "    \"\"\"Predicts grammatical acceptability score of sentence.\"\"\"\n",
    "    inputs = tokenizer(sentences, padding=True, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probas = torch.softmax(outputs.logits, dim=1).detach().cpu().numpy()\n",
    "    return probas[:, 1]\n",
    "    \n",
    "def grammatical_correctness(df, full_gen=False, batch_size=64):\n",
    "    # use complete generation instead of first sentence if full_gen=True\n",
    "    gen_pre_inj = []\n",
    "    gen_post_inj = []\n",
    "    count = 0\n",
    "    for n, row in df.iterrows():\n",
    "        if full_gen:\n",
    "            gen_pre_inj.append(row[\"gen\"])\n",
    "            gen_post_inj.append(row[\"gen_pipeline\"])\n",
    "        else:\n",
    "            gen_pre_inj.append(sent_tokenize(row[\"gen\"])[0])\n",
    "            gen_post_inj.append(sent_tokenize(row[\"gen_pipeline\"])[0])\n",
    "        if len(gen_pre_inj) == batch_size or n == len(df)-1:\n",
    "            grammar_pre_inj = predict_acceptability(gen_pre_inj, tokenizer, model)\n",
    "            grammar_post_inj = predict_acceptability(gen_post_inj, tokenizer, model)\n",
    "            count += np.sum((grammar_post_inj - grammar_pre_inj) / grammar_pre_inj)\n",
    "            gen_pre_inj = []\n",
    "            gen_post_inj = []\n",
    "    return count / len(df)\n",
    "\n",
    "np.round(grammatical_correctness(df) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb784bf-4e5d-4c21-a522-98e99171c891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_edits\n",
    "df[\"correction\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10212414-ded7-4282-9303-269b01d00edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing edits\n",
    "len(df[~df[\"acc\"] & df[\"correction\"] & df[\"acc_pipeline\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b122582-cb74-462b-a77a-5a345c0b89c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# breaking edits\n",
    "len(df[df[\"acc\"] & df[\"correction\"] & ~df[\"acc_pipeline\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc34e66-8e90-48c1-b058-b719f8aa93fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"t_gen\", df[\"t_gen\"].mean().round(2))\n",
    "print(\"t_extraction\", df[\"t_extraction\"].mean().round(2))\n",
    "print(\"t_retrieval\", df[\"t_retrieval\"].mean().round(2))\n",
    "print(\"t_injection\", df[\"t_injection\"].mean().round(2))\n",
    "print(\"t_pipeline\", df[\"t_pipeline\"].mean().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71a5393-3efa-4992-afe3-431fe5c61ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analizying pipeline results\n",
    "# wrong retrieval but correct in pipeline\n",
    "len(df[df[\"correction\"] & ~df[\"acc_retrieval\"] & df[\"acc_pipeline\"]])\n",
    "#df[df[\"correction\"] & ~df[\"acc_retrieval\"] & df[\"acc_pipeline\"]].to_excel(\"pipeline_wrong_retr_correct_pipe_ici.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2be58dc-e116-467a-8dd6-dcdc05e75f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "files = sorted([file for file in os.listdir(output_dir) if \"json\" in file])\n",
    "logger.info(f\"Found {len(files)} files in {output_dir}\")\n",
    "\n",
    "for n, fname in enumerate(files):\n",
    "    logger.info(f\"{n+1}/{len(files)} Evaluating {fname}\")\n",
    "    _, dataset, split, method, model_name = fname.split(\".\")[0].split(\"_\")\n",
    "\n",
    "    # load data\n",
    "    df_gen = pd.read_json(f\"{output_dir}/{fname}\", lines=True)\n",
    "    df_gt = pd.read_json(f\"../datasets/{dataset}/{dataset}_{split}.json\", lines=True)\n",
    "    df = df_gt.merge(df_gen)\n",
    "    wikidata_dict = load_wikidata_json(\"../datasets/wikidata_entity_data.json\")\n",
    "    logger.info(f\"Loaded dataframe with {len(df)} rows\")\n",
    "\n",
    "    df[\"acc\"] = df.apply(lambda row: accuracy(row), axis=1)\n",
    "    df[\"acc_pipeline\"] = df.apply(lambda row: accuracy_pipeline(row), axis=1)\n",
    "    df[\"acc_retrieval\"] = df.apply(lambda row: accuracy_retrieval(row), axis=1)\n",
    "    \n",
    "    results.append(\n",
    "        {\n",
    "            \"output_dir\": output_dir,\n",
    "            \"fname\": fname,\n",
    "            \"dataset\": dataset,\n",
    "            \"split\": split,\n",
    "            \"method\": method,\n",
    "            \"model_name\": model_name,\n",
    "            \"n_samples\": len(df),\n",
    "            \"acc\": df[\"acc\"].mean(),\n",
    "            \"acc_pipeline\": df[\"acc_pipeline\"].mean(),\n",
    "            \"acc_edit\": (~df[\"acc\"] == df[\"correction\"]).mean(),\n",
    "            \"acc_retrieval\": df[~df[\"acc\"] & df[\"correction\"]][\"acc_retrieval\"].mean(),\n",
    "            \"acc_retrieval_all\": df[df[\"correction\"]][\"acc_retrieval\"].mean(),\n",
    "            \"acc_injection\": df[~df[\"acc\"] & df[\"correction\"] & df[\"acc_retrieval\"]][\"acc_pipeline\"].mean(),\n",
    "            \"acc_injection_all\": df[df[\"correction\"]][\"acc_pipeline\"].mean(),\n",
    "            \"acc_retrieval_n_samples\": len(df[~df[\"acc\"] & df[\"correction\"] & df[\"acc_retrieval\"]]),\n",
    "            \"num_edits\": df[\"correction\"].sum(),\n",
    "            \"num_correct_edits\": len(df[df[\"correction\"] & df[\"acc_pipeline\"]]),\n",
    "            \"num_breaking_edits\": len(df[df[\"acc\"] & df[\"correction\"] & ~df[\"acc_pipeline\"]]),\n",
    "            \"fluency\": fluency_ngram_entropy(df),\n",
    "            \"grammar\": grammatical_correctness(df, full_gen=False),\n",
    "            \"t_gen\": df[\"t_gen\"].mean().round(2),\n",
    "            \"t_extraction\": df[\"t_extraction\"].mean().round(2),\n",
    "            \"t_retrieval\": df[\"t_retrieval\"].mean().round(2),\n",
    "            \"t_injection\": df[df[\"t_injection\"] != 0][\"t_injection\"].mean().round(2), # do not consider cases with no injection\n",
    "            \"t_injection_all\": df[\"t_injection\"].mean().round(2),\n",
    "            \"t_pipeline\": df[\"t_pipeline\"].mean().round(2)\n",
    "        }\n",
    "    )\n",
    "    logger.info(f\"Finished evaluation for file '{fname}'\")\n",
    "\n",
    "logger.info(f\"Finished evaluation for {output_dir}\")\n",
    "results_dir = f\"{output_dir}_eval.csv\"\n",
    "logger.info(f\"Writing results to {results_dir}\")\n",
    "pd.DataFrame(results).to_csv(results_dir, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
